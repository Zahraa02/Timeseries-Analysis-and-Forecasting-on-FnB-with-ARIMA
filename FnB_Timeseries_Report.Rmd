---
title: "Timeseries Analysis & Forecasting on FnB"
author: "Zahra Nur Anisah"
date: "`r Sys.Date()`"
output: 
 html_document:
   toc: true
   toc_float: 
      collapsed: true
   highlight: zenburn
   df_print: paged
   theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Case Intro

Customer behavior in the food and beverage industry is strongly influenced by seasonal patterns, which play a crucial role in shaping visitor trends. To optimize operations and make informed business decisions for 2018, the outlet owner aims to analyze visitor numbers across dine-in, delivery, and takeaway transactions. Timeseries analysis is well-suited for this purpose, offering accurate forecasting and insightful seasonality breakdowns.

To support this effort, a comprehensive dataset provided by Dattabot captures detailed transaction records from multiple food and beverage outlets over the past several months. Leveraging this dataset, the objective is to forecast visitor numbers and analyze hourly seasonality patterns for the days ahead. These insights will empower the outlet owner to anticipate demand effectively and implement data-driven strategies.

This report presents the forecasted hourly visitor counts for the next 7 days, spanning Monday, February 19th, 2018, to Sunday, February 25th, 2018, along with a detailed interpretation of seasonal trends.


# Library

Before building the model, it's essential to load the necessary libraries to ensure seamless execution.

```{r, message=FALSE, warning=FALSE}
# load library

library(dplyr) # data wrangling
library(lubridate) # date manipulation
library(padr) # complete data frame
library(zoo) # missing value imputation
library(forecast) # timeseries library
library(TTR) # for Simple moving average function
library(MLmetrics) # calculate error
library(tseries) # adf.test
library(fpp) # data for forecasting: principles and practice
library(TSstudio) # timeseries visualization
library(ggplot2)
library(tidyr)
```


# Import & Data Wrangling

Here is the training dataset that contains detailed transaction information over the last couple of months, from December 1st, 2017, to February 18th, 2018.

**Info**: The shop opens from 10 am to 10 pm every day

```{r}
# train dataset
fnb_train <- read.csv("datasets/data-train.csv")
fnb_train
```

The dataset includes information about:

 * `transaction_date`: The timestamp of a transaction
 * `receipt_number`: The ID of a transaction
 * `item_id`: The ID of an item in a transaction
 * `item_group`: The group ID of an item in a transaction
 * `item_major_group`: The major-group ID of an item in a transaction
 * `quantity`: The quantity of purchased item
 * `price_usd`: The price of purchased item
 * `total_usd`: The total price of purchased item
 * `payment_type`: The payment method
 * `sales_type`: The sales method

And here is the test dataset, which contains the correct format for the expected forcasted results. 

```{r}
# test dataset
fnb_test <- read.csv("datasets/data-test.csv")
fnb_test
```

The test dataset consists of:

  * `datetime`: Timestamp, covering the next 7 days from Monday, February 19th, 2018, to Sunday, February 25th, 2018.
  * `visitor`: Estimated number of visitors.

It is important to note that the `visitor` column is currently empty and will be filled with the results generated by the timeseries and forecast model I develop.


# Data Preprocessing

There are a few steps that need to be completed before I can start using the data and converting it into timeseries data, that process is called Data Preprocessing. The essential steps that must be completed before I can convert the training data into timeseries data are:

  1. Ensure the time period data is in the correct order and the appropriate data type. Since the focus is on hourly frequency, the data must be in the `POSIXct` format. 
  2. Verify that no dates are missing and that all intervals are consistent.
  3. Ensure there are no missing values in the target variable.


This is the stage where I begin the process of preprocessing my `fnb_train` dataset, cleaning it, transforming it, and organizing the dataset to ensure it is ready for analysis and modeling:

**1. Data must be in order and the appropriate format**

Here I would like to:

- Change the time period data to the appropriate format.
- Extract the hourly data for further use.
- Filter the data only to use the data that are from the shop opening hours (10 am to 10 pm).
- Aggregate/summarise the data to acquire the number of visitors each hour for each day.
- Arrange the data based on the date and hourly order.

```{r, warning=FALSE, message=FALSE}
fnb_train <- fnb_train %>% 
  mutate(
    transaction_date = ymd_hms(transaction_date),
    hour = hour(transaction_date),
    transaction_date = as.Date(transaction_date)) %>% 
  filter(hour %in% 10:22) %>% 
  group_by(transaction_date, hour) %>% 
  summarise(visitors = n_distinct(receipt_number)) %>% 
  ungroup() %>% 
  arrange(transaction_date, hour) 

fnb_train
```

Now that I have changed the data type for `transaction_date`, extracted the hour, filtered to shop's operating hours, and calculated the number of visitors for each hour of each day, it's time to verify that there are no missing dates in the data and ensure that all time intervals are consistent.

**2. Verify no missing dates and all the intervals are constant**

After obtaining the `fnb_train` data above, it is essential to ensure that no dates are missed and that all intervals are consistent. To achieve this:

- First, I need to create a reference dataset containing a complete timeseries from the earliest date and hour to the latest date and hour based on the `fnb_train` data.
```{r}
# Create complete date timeseries 
time_range <- seq.Date(from =min(fnb_train$transaction_date), 
                           to =max(fnb_train$transaction_date),
                           by = "day" )

# Create complete hourly timeseries based on valid shop opening hours (10 am to 10 pm)
valid_hours <- c(10:22)

# Combine those two timeseries to create complete date and hourly timeseries dataset
complete_time <- expand.grid(
  transaction_date = time_range,
  hour = valid_hours
)

complete_time
```
Based on the `complete_time` dataset, which includes all dates and hours within the training data time period, the complete dataset should contain 1,040 entries/ data. This total is calculated from 13 hours per day Ã— 80 days = 1,040 entries. However, I noticed that my `fnb_train` data contains only 1,004 entries, indicating that 36 entries are missing.

- Second, I need to identify the specific dates and hours that are missing.
```{r}
# Find the missing date and hour
missing_date <- complete_time %>% 
  anti_join(fnb_train, by = c("transaction_date", "hour"))

missing_date
```
From the `missing_date` dataset, I can identify all the dates and hours that are missing from my `fnb_train` dataset.

- Third, I need to add the missing dates and hours back into my `fnb_train` dataset to ensure the time period is complete and continuous.
```{r}
fnb_train <- bind_rows(fnb_train, missing_date) %>% arrange(transaction_date, hour)

fnb_train
```
Now that the time period is complete, I can see that there are a few NA values in the `fnb_train` dataset. To ensure there are no missing values in the target variable/ `visitors`, I first need to check how many data has missing values.


**3. Ensure there are no missing values in the target variable**

To check how many data that have NA/ missing value, I can check with the code below.
```{r}
# Check the amount of data with missing value
fnb_train %>% is.na() %>% colSums()
```
It is evident that there are 36 entries with missing values, which corresponds to the number of data added to the `fnb_train` dataset. To address this, I will fill the missing values with `0`, as it can be assumed that if there is no transaction history for a specific date and hour, no visitors came to the shop during that time. This process of filling missing values is known as padding.
```{r}
# Pad the missing value with the value "0"
fnb_train <- fnb_train %>% 
  mutate(visitors = na.fill(object = visitors,
                            fill = "0"))
fnb_train
```

Now, if I check the missing value again, there should no longer be any data containing NA or missing values.
```{r}
# Check the amount of data with missing value again
fnb_train %>% is.na() %>% colSums()
```
Now that there are no missing values and both the time period data and the target variable are complete, the final step is to combine the `hour` with the `transaction_date` and convert it into the appropriate `POSIXct` format. This ensures that the training data is in the same format as the test data.
```{r}
# Create a datetime column
fnb_train <- fnb_train %>%
  mutate(
    transaction_date = as.POSIXct(
      paste(transaction_date, sprintf("%02d:00:00", hour)),
      format = "%Y-%m-%d %H:%M:%S")) %>%
  select(transaction_date, visitors) 

fnb_train
```
Now that the preprocessing is complete and the training data (`fnb_train`) is ready, the next step is to convert the data into a timeseries object, so it can be used for forecasting.


# Creating TS Object with `ts()`

Since the focus of this case is on the total number of visitors per hour during the shop's daily opening hours, the `frequency` should reflect the number of data recorded each day.

As the shop operates for 13 hours daily, from 10 am to 10 pm, I used 13 as the `frequency` for the timeseries object.
```{r}
# Convert training dataset to timeseries object
visitor_ts <- ts(data = fnb_train$visitors, 
               frequency = 13) 
```

```{r}
# Visualize the timeseries object
autoplot(visitor_ts) 
```
The timeseries plot above shows that the timestamp ranges from 0 to 80, representing the total number of days in the `fnb_train` dataset (80 days). Each timestamp corresponds to one day and includes 13 visitor values, reflecting the 13 operating hours of the shop each day.
 
## Decomposition: Timeseries Object

Now, it's time to analyze the trend, seasonality, and remainder of the timeseries object.
```{r}
# Decompose: timeseries object
visitor_ts_dc <- visitor_ts %>% 
  decompose(type = "additive")

# Visualize decompose: timeseries object
visitor_ts %>% 
  tail(13*7*4) %>% 
  decompose() %>% 
  autoplot()
```

Plot's Insight: 

- Trend Component: The trend is not clearly visible, as it appears to capture additional patterns or potentially overlooked seasonal variations.
- This suggests that the dataset likely includes multiple seasonal patterns.
- To address this, it is crucial to use a Multiple Seasonality Time Series (MSTS) format, which allows for the seamless incorporation of multiple frequency settings to better represent the data.


# Creating MSTS Object with `msts()`

The primary difference between using an MSTS object and a regular TS object lies in the handling of multiple seasonal frequencies. When creating an MSTS object, the `seasonal.periods` parameter specifies the different frequencies in the data. For this case, it includes:

  * Daily seasonality: 13 hours (representing the shop's daily operating hours).
  * Weekly seasonality: 13 hours Ã— 7 days (representing a full week of operations).
  
This approach allows for the modeling of both daily and weekly patterns within the dataset.
```{r}
visitor_msts <- msts(data = fnb_train$visitors,
                     seasonal.periods = c(13, 13*7))
```

```{r}
# Visualize the multi-seasonality time series object
autoplot(visitor_msts) 
```
The plot above represents the multi-seasonality time series (MSTS) of the `fnb_train` dataset. The timestamps range from 0 to 12, with each timestamp corresponding to one week. This range reflects the dataset's total duration of approximately 12 weeks, calculated from 80 days (80 Ã· 7 â‰ˆ 11.4 weeks). Each timestamp/ week is composed of 7 days, and each day contains 13 visitor values, representing the 13 operating hours of the shop. This structure highlights the dataset's multi-seasonal nature, capturing patterns across daily, weekly, and hourly time frames.

## Decomposition: Multi-Seasonality Time Series Object

Now, it's time to further analyze the trend, seasonality, and remainder of the multi-seasonality time series object.
```{r}
# Decompose: MS time series object
visitor_msts_dc <- visitor_msts %>% 
  mstl() 

# Visualize decompose: MS time series object
visitor_msts %>% 
  tail(13*7*4) %>% 
  stl(s.window = "periodic") %>% 
  autoplot()
```

Plot's Insight: 

- Trend Component: In this multi-seasonality time series object, the trend is clearly visible, showcasing the overall movement or direction in the data over time. It highlights gradual changes in the average visitor count, which may represent long-term shifts in customer behavior.


# Seasonality Analysis 

## Hourly Seasonality Analysis

This section focuses on creating a visualization and analyzing the hourly seasonality patterns in the data. The goal is to identify recurring trends or patterns in visitor counts based on the time of day.
```{r}
fnb_train %>% 
  mutate(Hour = hour(transaction_date),
         Seasonal = visitor_ts_dc$seasonal) %>% 
  distinct(Hour, Seasonal) %>% 
  ggplot(mapping = aes(x = Hour,
                       y = Seasonal, 
                       fill = Seasonal)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "darkgreen", high = "green", name = "Seasonal") + 
  scale_x_continuous(breaks = seq(10,22,1)) +
  labs(
    title = "Seasonality Analysis ",
    subtitle = "Daily by Hour",
    x = "Hour",
    y = "Seasonal") +
  theme_minimal()
```

Hourly Seasonality Insights:

- Peak Visitor Hours: The majority of visitors arrive between 7:00 PM and 10:00 PM, indicating that these are the busiest hours for the store. This is likely due to dinner-time traffic or post-work activities.
- Least Visitor Hours: Visitor counts are lowest during the opening hours at 10:00 AM, which might align with a general tendency for customers to avoid visiting early in the day.
  
  
## Weekly Seasonality Analysis

In this section, I will create a visualization and conduct an analysis of the weekly and hourly seasonality patterns in the data. The objective is to uncover trends and recurring patterns in visitor behavior throughout the week and at various times of the day.
```{r}
as.data.frame(visitor_msts_dc) %>% 
  mutate(transaction_date = fnb_train$transaction_date) %>% 
  mutate(Day_of_Week = wday(transaction_date, 
                            label = T, 
                            abbr = F),
         Hour = as.factor(hour(transaction_date))) %>% 
  group_by(Day_of_Week, Hour) %>% 
  summarise(Seasonal = sum(Seasonal13 + Seasonal91)) %>% 
  ggplot() +
  geom_bar(aes(x = Hour, 
               y = Seasonal, 
               fill = Day_of_Week), 
           stat ="identity", 
           position = "stack") +
  scale_fill_manual(values = c("Monday" = "red", "Tuesday" = "orange", "Wednesday" = "yellow", "Thursday" ="green","Friday" = "blue", "Saturday" = "darkgreen", "Sunday" = "purple" )) +
  labs(
    title = "Multi Seasonality Analysis ",
    subtitle = "Weekly by Hour"
  )
```

Weekly Seasonality Insights:

- Peak Visitor Hours: The highest seasonal values occur between 7:00 PM and 10:00 PM, particularly on weekend evenings (Friday, Saturday, and Sunday), indicating these hours are the busiest for the business.
- Low Visitor Hours: The lowest seasonal values are observed during the opening hours (10:00 AM - 12:00 PM). This is consistent across all days of the week, suggesting that mornings see minimal customer activity.
- Weekly Seasonal Variation: The chart confirms that weekend evenings (Friday-Sunday) have the most pronounced seasonal patterns compared to weekdays. This reflects a common trend in food and beverage outlets where customer activity spikes during the weekend.
- Business Implications:
  * These patterns suggest that the business may need to allocate more staff and resources during peak hours, especially on Friday and Saturday evenings, to cater to increased customer demand.
  * Promotions or offers during off-peak hours (morning to early afternoon) could help balance the flow of visitors throughout the day.
    

# Cross Validation

In this section, I will perform a cross-validation or train-test splitting:

- **Train data** will consist of the initial portion of the `visitor_msts` dataset.

- **Test data** will consist of the most recent portion of the `visitor_msts` dataset.

```{r}
# Subsetting Test Data
# Take the data of the last 7 days/ the last 91 data 
visitor_msts_test <- visitor_msts %>% tail(91)
```

```{r}
# Subsetting Train Data
# Take everything EXCEPT the data of the last 7 days/ the last 91 data 
visitor_msts_train <- visitor_msts %>% head(-91) 
```

```{r}
# Visualize Train Data & Test Data 
autoplot(visitor_msts_train, series = "Train Data") + 
  autolayer(visitor_msts_test, series = "Test Data")
```
The plot above shows the `visitor_msts` dataset divided into two parts, the training dataset and the testing dataset.


# Modeling

In this section, I will develop three models and compare their performance using Mean Absolute Error (MAE) as the evaluation metric. The models I will build are as follows:

1. **Triple Exponential Smoothing** (Holt-Winters Exponential) 
2. **TBATS** (Trigonometric, Box-Cox Transformation, ARMA Errors, Trend, and Seasonal Components)
3. **ARIMA** (AutoRegressive Integrated Moving Average)


## Model 1: Triple Exponential Smoothing 

```{r, message=FALSE, warning=FALSE}
# Model 1: Triple Exponential Smoothing (Holt-Winters Exponential)
model_hw <- HoltWinters(visitor_msts_train) 

# Forecast Triple Exponential Smoothing
forecast_hw <- forecast(object = model_hw, 
                        h = 91) 
```


```{r}
# Visualize the forecast result of HW model 
visitor_msts_train %>% 
  autoplot() +
  autolayer(visitor_msts_test, series = "Test Data") + 
  autolayer(model_hw$fitted[,1], series =  "Model") +  # Visualisasi hasil prediksi model terhadap/ pada data train
  autolayer(forecast_hw$mean, series = "Forecast") # Visualisasi hasil forecast/ prediksi pada data test
```


```{r}
# Evaluate HW Model
accuracy(forecast_hw$mean, visitor_msts_test) 
```

From the plot and the MAE values, it is evident that the accuracy of **Triple Exponential Smoothing** is reflected in its MAE: **6.463898**.

## Model 2: TBATS

```{r}
# Model 2: TBATS
tbats_model <- tbats(visitor_msts_train)

# Forecast TBATS
tbats_forecast <- forecast(tbats_model,
                           h = 91)
```

```{r}
# Visualize the forecast result of TBATS model
visitor_msts_train %>% 
  autoplot() + 
  autolayer(visitor_msts_test, series = "Test Data") +  
  autolayer(tbats_model$fitted, series = "Model") + 
  autolayer(tbats_forecast$mean, series = "Forecast") 
```

```{r}
# Evaluate TBATS Model
accuracy(tbats_forecast$mean, visitor_msts_test)
```
From the plot and the MAE values, it is clear that **TBATS** achieves an MAE of **6.184045**, demonstrating better performance and accuracy compared to the Triple Exponential Smoothing model.

## Model 3: ARIMA

```{r}
# Model 3: Arima
model_arima <- stlm(visitor_msts_train,
                     method = "arima")

# Forecast Arima
forecast_arima <- forecast(model_arima, 
                           h = 91)
```


```{r}
# Visualize the forecast result of Arima model 
visitor_msts_train %>% 
  autoplot() + 
  autolayer(visitor_msts_test, series = "Test Data") +  
  autolayer(model_arima$fitted, series = "Model") + 
  autolayer(forecast_arima$mean, series = "Forecast") 
```


```{r}
# Evaluate ARIMA Model
accuracy(forecast_arima$mean, visitor_msts_test) 
```
The plot and MAE values above indicate that **ARIMA** achieves an MAE of **5.688691**, demonstrating a smaller MAE and an even better performance and accuracy compared to the TBATS model.

## Models Evaluation

Here I will rank the best model based on the lowest MAE.
```{r}
# Rank the best model based on the lowest MAE 
MAE(forecast_hw$mean, visitor_msts_test) # HW Model
MAE(tbats_forecast$mean, visitor_msts_test) # TBATS Model
MAE(forecast_arima$mean, visitor_msts_test) # Arima Model
```
Based on the MAE results, it is evident that **ARIMA** has the **lowest MAE**. Therefore, ARIMA is identified as the **best performing model**, providing the **highest accuracy** for this case.


# Visualization Actual vs Estimated

To create a visualization comparing the performance of all three models, I will first construct a dataframe containing the forecasted results for each model along with their respective actual values.
```{r}
# The collection of actual data & forecasted results 
accuracy_dataframe <- data.frame(
  transaction_date = fnb_train$transaction_date %>% tail(13*7),
  actual = as.vector(visitor_msts_test),
  HW = as.vector(forecast_hw$mean),
  TBATS = as.vector(tbats_forecast$mean),
  Arima = as.vector(forecast_arima$mean)
)
accuracy_dataframe
```

The first visualization will display the actual test data compared to the forecasted results generated by the **ARIMA** model, highlighting its superior performance as the **best-performing model**. This plot will visually emphasize how closely the ARIMA model predictions align with the actual data, showcasing its accuracy.
```{r, message=FALSE, warning=FALSE}
# Visualization of Actual vs Estimated (Best Model: Arima)
accuracy_dataframe %>% 
  ggplot() +
  geom_line(aes(x = transaction_date, 
                y = actual, 
                colour = "Actual"),
            size = 1) +
  geom_line(aes(x = transaction_date, 
                y = Arima, 
                colour = "Arima Model (Best Model)"),
            size = 1)+
  labs(
    title = "Actual vs Arima Model",
    subtitle = "Hourly Visitors in Test Data", 
    x = "Date", 
    y = "Visitor"
    ) +
  scale_color_manual(values = c("Actual" = "red",
                                "Arima Model (Best Model)" = "blue"))
```

This visualization compares the actual test data with the forecasted results from all three models, **Triple Exponential Smoothing**, **TBATS**, and **ARIMA**. By presenting all models in a single plot, it highlights the differences in their predictive accuracy and illustrates how closely each model's forecasts align with the actual data.
```{r, message=FALSE, warning=FALSE}
# Visualization of Actual vs All Estimated Data
accuracy_dataframe %>% 
  ggplot() +
  geom_line(aes(x = transaction_date, 
                y = actual, 
                colour = "Actual"),
            size = 0.5) +
  geom_line(aes(x = transaction_date, 
                y = Arima, 
                colour = "Arima Model (Best Model)"),
            size = 0.5) +
  geom_line(aes(x = transaction_date, 
                y = HW, 
                colour = "Holt-Winters Exponential"),
            size = 0.5) +
  geom_line(aes(x = transaction_date, 
                y = TBATS, 
                colour = "TBATS"),
            size = 0.5) +
  labs(
    title = "Actual vs All Model",
    subtitle = "Hourly Visitors in Test Data", 
    x = "Date", 
    y = "Visitor"
    ) +
  scale_color_manual(values = c("Actual" = "blue",
                                "Arima Model (Best Model)" = "red",
                                "Holt-Winters Exponential"= "green",
                                "TBATS" = "orange" ))
```


# Predict Performance

Based on the earlier model evaluation, **ARIMA** is identified as the **best-performing model** due to its **lowest MAE**, delivering the **highest accuracy** in this case. Furthermore, ARIMA is the only model to achieve an **MAE of less than 6**, meeting the performance standards for predictions.  

Therefore, I will use the ARIMA model to forecast the visitor numbers for a new test dataset, which includes the next 7 days following the last date in the `fnb_train` dataset. Here I will take a look again at the test data.
```{r}
# test data
fnb_test
```

To maintain consistency with the `fnb_train` data, I will convert the `datetime` values to the `POSIXct` format. This step ensures compatibility and facilitates a smoother workflow when handling and analyzing the data.
```{r}
# Change the datetime format to POSIXct
fnb_test <- fnb_test %>% 
  mutate(
    datetime = ymd_hms(datetime)
  )
fnb_test
```

With the test data (`fnb_test`) now formatted to match the training data (`fnb_train`), I will proceed to use the **ARIMA** model to generate forecasts for the test data. 
```{r}
# Model: ARIMA
Arima <- stlm(visitor_msts,
              method = "arima")
```

Since the `fnb_test` data covers one week, I will forecast the next 91 entries, corresponding to 13 hours per day over 7 days (13 Ã— 7 = 91). 
```{r}
# Forecast test data
forecast_mod <- forecast(Arima,
                         h = 91)
```

Next, I will update the `visitor` column in the `fnb_test` dataset with the forecasted results from the ARIMA model. The updated dataset will be saved as an object named `submission`.
```{r}
# save forecasted result to the column visitor
submission <- fnb_test %>% 
  mutate(visitor = forecast_mod$mean)
submission
```

This will export the `submission` data to a csv file.
```{r}
# save data
write.csv(submission, "Submission-Zahra.csv", row.names = F)
```

And here is the result when I read the `submission` csv file.
```{r}
# read the newly saved csv
read.csv("Submission-Zahra.csv")
```

Here is the accuracy result of the test data, or the `submission` CSV file, after checking the MAE accuracy on the Algoritma Capstone Leaderboard. 
```{r echo=FALSE}
knitr::include_graphics("Test_data_MAE_result2.jpg")
```


# Conclusion

Lastly, I will perform two assumption checks on my model:  

**1. No Autocorrelation in Residuals: **  
   Ensuring that the residuals are not autocorrelated, which would indicate that the model has captured all patterns in the data.  

```{r}
# No-autocorrelation residual
Box.test(forecast_arima$residuals, 
         type = "Ljung-Box")
```
When using the `Box.test` to check for autocorrelation in the residuals, the p-value of 0.9883, being greater than the alpha level of 0.05, indicates that the model **does not have any autocorrelation** in its residuals.


**2. Normality of Residuals:**  
   Verifying that the residuals follow a normal distribution, which is an assumption for many statistical models to ensure reliable predictions.

```{r}
#  Normality of Residual
shapiro.test(forecast_arima$residuals)
```
When checking the model's residuals for normality using the `shapiro.test`, the p-value of 8.6e-06 is less than the alpha level of 0.05. This indicates that the **residuals are not normally distributed**. In other words, the residuals still forms some sort of pattern.

**This may occur because:**

- The dataset may not be large enough, which can affect the distribution of residuals. However, this does not imply that the forecast results are inaccurate, as the MAE used for model evaluation still indicates strong accuracy.

**To adddress this issue:**

- Gather more data to improve the model's training process, allowing residuals to become more normally distributed. This can enhance model evaluation metrics and lead to better forecast accuracy.


# Summary

Based on the seasonality analysis, I can summarize that:

- Peak Visitor Hours: The highest visitor counts occur daily between **7:00 PM and 10:00 PM**, with significantly higher numbers during weekend evenings (Friday, Saturday, and Sunday). These hours represent the store's peak activity times, likely driven by dinner-time traffic and post-work activities, which are more frequent on weekends.
- Lowest Visitor Hours: The least busy hours are during the opening period at **10:00 AM**, with minimal activity observed between **10:00 AM and 12:00 PM** across all days of the week. This suggests that these early hours see fewer visitors, potentially due to lower demand for store services during the morning.
- Business implications:

  - The business should allocate additional staff and resources between **7:00 PM and 10:00 PM**, especially on **Friday, Saturday, and Sunday evenings**, to manage the higher customer demand during these peak hours. This includes ensuring adequate inventory, efficient service, and enhanced customer experience to capitalize on the increased traffic.
  - To boost visitor numbers during off-peak hours (10:00 AM to 12:00 PM), the business could introduce targeted promotions or discounts, such as **morning specials**, **early-bird offers**, or **loyalty rewards**. This approach can help attract more customers during the quieter periods and create a more balanced flow of visitors throughout the day.